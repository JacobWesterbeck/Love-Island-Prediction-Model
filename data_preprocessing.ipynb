{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "**********\n",
    "In this notebook I am will prepare the data from 'li_cleandata.csv' for modeling. This notebook will contain feature selection, feature engineering and creation of train/validate/test splits as well as explanations for how I want to use this data in my model.\n",
    "\n",
    "#### Defining Success\n",
    "- The purpose of this project is to determine if a candidate will be successful based on their features.\n",
    "- To evalute success, I will use the binary 'Finalist' feature.\n",
    "- Each season, 8 candidates (4 boys and 4 girls) make it to the shows finale. These 'Finalist' candidates will be our benchmark for success on the show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Outline:\n",
    "1. Import necessary packages/libraries\n",
    "2. Import Data\n",
    "3. Feature Selection\n",
    "4. Feature Engineering\n",
    "   - Loyalty Score\n",
    "   - Region \n",
    "5. Encode Categorical Data\n",
    "6. Data Splits\n",
    "7. Feature Scaling (Another iteration of model perhaps - not needed for logisitc regression but possibly usefulf or decision tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Will need to install scikit-learn\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(cwd + '/data/li_cleandata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection \n",
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 72 total finalists in this dataset (8 each season).\n",
    "\n",
    "- Therefore, 72 is the historical count of 'events' that my model is attempting to predict\n",
    "- I will limit myself to allowing roughly 1 variable per 15 'events' to prevent the model from becoming too complex or overfitting .\n",
    "- Using this strategy, I will try to limit myself to ~5 predictive variables (72/15 = 4.8).\n",
    "- I may go up to 6 or 7 total variables after some testing and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My target variable that the model will be predicting is 'Finalist'.\n",
    "- Having my target variable be a binary 'Yes' or 'No' is nice for simplicity sake and allows me to capture the essence of what I believe success to be on the show, which is make it to the last day of the show.\n",
    "\n",
    "Other target variables I could have used are:\n",
    "- Status: which gives the placements, 1-4, of each finalist. I decided against using this for a few reasons. Once you are a finalist, your placement 1-4 is completely dependant onthe public, whereas  making it to the finals is a combination of contestant voting themselves, inability to form a couple, and public voting. Basically I determined that if you can endure all factors of the show to reach the finale, you are successful and gives you a chance to place first. \n",
    "- Stay: I could have equated success on the show with how long one remains in the villa, but that would not account for people who were finalists and came on the show late, so this didn't make sense.\n",
    "\n",
    "\n",
    "The features that I believe to be most impactful in determining outcome of 'Finalist' are:\n",
    "- Age\n",
    "- Height\n",
    "- Hometown and/or Region\n",
    "- Entered\n",
    "- OG\n",
    "- Casa\n",
    "- Couples\n",
    "\n",
    "Entered, OG, and Casa are all capturing similar data. \n",
    "- I will probably decide between either just 'Entered' or use both 'OG' and 'Casa'\n",
    "\n",
    "\n",
    "I will do some feature engineering below to incorporate the features above in the most efficient way possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Couples\n",
    "The purpose of this feature is to determine if loyalty to one partner enhances chances of success, as opposed to coupling up with multiple partners.\n",
    "\n",
    "Loyalty is a word constantly mentioned on the show and I wanted to quantify a contestants loyalty, so I gathered data on each contestants number unique couplings to evalute loyalty.\n",
    "\n",
    "After analyzing this feature, I noticed some potention flaws in how this data point might be interpreted by the model.\n",
    "- Some contestants will have 0 couples, making it impossible to reach the finale if their 'Couples' value is 0.\n",
    "- Contestants who enter the show late and become finalists are likely to have a lower number of couples.\n",
    "\n",
    "A way to add more context to this feature would be to divide a candidates 'Couples' by their 'Stay' in the villa. \n",
    "- This will result in providing a value that quantifies a candidates loyalty during their time on the show.\n",
    "\n",
    "I will name this value a candidates 'Loyalty Score'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Loyalty Score\n",
    "data['Loyalty'] = (data['Couples'] / data['Stay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two issues with Loyalty Score:\n",
    "1. Malia Arkian has a stay of zero days so I will have to replace her Loyalty value of NaN. I will replace it with '1' to indicate that she had no loyatly.\n",
    "2. Candidates with 0 couples are given a loyalty score of 0. Since a lower loyalty score indicates a candidate was very loyal, I will want to change all instances of candidates with 0 couples to having a loyalty score  of 1, since they were unable to display any sort of loyalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contestants with 0 couples loyalty score set to 0\n",
    "data.loc[data['Couples'] == 0, 'Loyalty'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check loyalty range\n",
    "data['Loyalty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regions\n",
    "\n",
    "The purpose of this feature is to determine if candidates from a certain area perform better on the show. \n",
    "Analysis provided some insight but I want to consolidate the regions into a handful of categories to see if I can find a more informative trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List regions included in the dataset currently\n",
    "print(data['Region'].nunique())\n",
    "print(data['Region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find number of total distinct hometowns in this dataset\n",
    "print(data['Hometown'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset includes candidates from 8 unique regions and 124 unique hometowns. How will I consolidate this into one feature?\n",
    "- There are 2 contestants from Australia - I will make a category called something like'NFA' which will contain any candidates not from the UK or areas around it.\n",
    "- The rest of the contestants are from the same general area, how best to split that up?\n",
    "\n",
    "The United Kingdom of Great Britain and Northern Ireland (to give its full name) refers to the political union between England, Wales, Scotland and Northern Ireland.\n",
    "England is divided into 9 geographical regions. These are London, the North East, North West, Yorkshire, East Midlands, West Midlands, South East, East of England and the South West\n",
    "\n",
    "Regions I will use:\n",
    "- NFR (Not from Region - candidates from australia and any future candidates from australia or other areas like USA etc)\n",
    "- Surrounding Islands (will encapsulate contestants from british isles, isle of man or any other smaller islands surrounding the main island of UK)\n",
    "- Scotland\n",
    "- Ireland (will include Ireland and Northern Ireland)\n",
    "- Wales\n",
    "- England"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got 640 prominent UK cities and their latitudes from: https://simplemaps.com/data/gb-cities\n",
    "North: UK cities above 54 lat\n",
    "Midlands: UK cities between 52 and 54 lat\n",
    "South: UK cities below  52 lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported latitude of 680 English cities to help seperate contestant locations into regions\n",
    "eng_city_data = pd.read_csv(cwd + '/data/english_city_data.csv')\n",
    "eng_city_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check new dataframe\n",
    "eng_city_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split cities into regions by latitude\n",
    "eng_north = eng_city_data[eng_city_data['lat']>= 54]\n",
    "eng_mid = eng_city_data[eng_city_data['lat'].between(52,  54)]\n",
    "eng_south = eng_city_data[eng_city_data['lat']<= 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the above into lists\n",
    "north = eng_north['city'].tolist()\n",
    "midlands = eng_mid['city'].tolist()\n",
    "south = eng_south['city'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty column for location\n",
    "data = data.assign(Location=\" \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If region is not england, I will copy that value to the Location column\n",
    "data['Location'] = np.where(~data['Region'].isin(['England']), data['Region'], data['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If region IS england, I will copy the value of 'Hometown' to the 'Location' column\n",
    "data['Location'] = np.where(data['Region'].isin(['England']), data['Hometown'], data['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dict to map possible values to keys\n",
    "location_map = {\n",
    "    'North' : north,\n",
    "    'Midlands' : midlands,\n",
    "    'South' : south,\n",
    "    'Wales' : ['Wales'],\n",
    "    'Scotland' : ['Scotland'],\n",
    "    'Ireland' : ['Northern Ireland', 'Ireland'],\n",
    "    'Islands' : ['Isle of Man', 'British Isles'],\n",
    "    'NFR' : ['Australia']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert the map for easier use\n",
    "invert_map = {}\n",
    "for key, value  in location_map.items():\n",
    "    for v in value:\n",
    "        invert_map.setdefault(v, []).append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the mapping \n",
    "data['Final_Location'] = data['Location'].map(invert_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where mapping didn't work \n",
    "# 135 rows received a Nan value\n",
    "data_locay_chk = data[data['Final_Location'].isnull()]\n",
    "data_locay_chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the amount of unique values that were not mapped\n",
    "#Print the names of those values\n",
    "\n",
    "print(data_locay_chk.Location.nunique())\n",
    "print(data_locay_chk.Location.unique())\n",
    "\n",
    "#59 unique values were not mapped properly\n",
    "#I will add the 59 values below to my dataset and reupdate locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checked latitudes of above list and categorized them into North/Midlands/South\n",
    "# 19 in north\n",
    "# 13 in mid\n",
    "# 27 in south\n",
    "\n",
    "north_additions = ['Wakefield', 'Newcastle', 'Sheffield', 'Barnsley', 'York', 'Wigan', 'Leeds', 'Derbyshire', 'Preston', 'Doncaster', 'Northumberland', 'Kingston upon Hull', 'Hull', 'Middlesbrough', 'Bedford', 'Huddersfield', 'Redcar', 'Bolton', 'Cumbria']\n",
    "midlands_additions = ['Wilmslow', 'Teddington', 'Cheltenham', 'Swindon', 'Ipswich', 'Cotswolds', 'Bromley', 'Reading', 'Chelsea', 'Battersea', 'Warwickshire', 'Thrisk', 'Milton Keynes']\n",
    "south_additions = ['Cheshire', 'Hertfordshire', 'Brighton', 'Wiltshire', 'Essex', 'Surrey', 'Yarm', 'Cornwall', 'Pett', 'North London', 'Kingston', 'Hampshire', 'Cockfosters', 'Devon', 'The Wirral', 'Buckinghamshire', 'Exeter', 'South East London', 'Suffolk', 'South London', 'Worthing', 'East London', 'Guildford', 'Shropshire', 'Staffordshire', 'Kent', 'Tring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the above to north/midlands/south variables by using the .extend() method\n",
    "north2 = north + north_additions\n",
    "midlands2 = midlands + midlands_additions\n",
    "south2 = south + south_additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.assign(Location=\" \", Final_Location=\" \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If region is not england, I will copy that value to the Location column\n",
    "data['Location'] = np.where(~data['Region'].isin(['England']), data['Region'], data['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If region IS england, I will copy the value of 'Hometown' to the 'Location' column\n",
    "data['Location'] = np.where(data['Region'].isin(['England']), data['Hometown'], data['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dict to map possible values to keys\n",
    "location_map2 = {\n",
    "    'North' : north2,\n",
    "    'Midlands' : midlands2,\n",
    "    'South' : south2,\n",
    "    'Wales' : ['Wales'],\n",
    "    'Scotland' : ['Scotland'],\n",
    "    'Ireland' : ['Northern Ireland', 'Ireland'],\n",
    "    'Islands' : ['Isle of Man', 'British Isles'],\n",
    "    'NFR' : ['Australia']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert the map for easier use\n",
    "invert_map2 = {}\n",
    "for key, value in location_map2.items():\n",
    "    for v in value:\n",
    "        invert_map2.setdefault(v, []).append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the mapping \n",
    "data['Final_Location2'] = data['Location'].map(invert_map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where mapping didn't work \n",
    "# 135 rows received a Nan value\n",
    "data_local_chk2 = data[data['Final_Location2'].isnull()]\n",
    "data_local_chk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The values were saved as lists with 1 value, so I indexed to the first location and set the column value equal to the first(and only) string in the list\n",
    "#This effectively removes the brackets\n",
    "data['Final_Location2'] =  data['Final_Location2'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVERY LOCATION IS ACCOUNTED FOR!!!!\n",
    "8 categories for a contestants location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original dataset contains 680 cities/towns\n",
    "- Manually added 59 counties\n",
    "- This gives me a range of 739 possible values that I can receive via love island wiki and programatically map.\n",
    "- Was tedious work up front but I should be set up to add new contestants and have them categorized by region without any further manual work. \n",
    "- I can add any cities/counties I missed as they arise but hopefully with 739 mapped values, I am covered from having to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of New Features\n",
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to analyze loyalty score and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package for quick analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some dataframes for quick analysis\n",
    "\n",
    "#split data by Finalists to analyze new features\n",
    "finalists = data[data['Finalist'] == 'Yes']\n",
    "non_finalists = data[data['Finalist'] == 'No']\n",
    "\n",
    "# Split data by values in Status as well\n",
    "first = data[data['Status'] == '1']\n",
    "second = data[data['Status'] == '2']\n",
    "third = data[data['Status'] == '3']\n",
    "fourth = data[data['Status'] == '4']\n",
    "dumped = data[data['Status'] == 'Dumped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average loyalty score for each 'Status' value\n",
    "print('1st Place Contestants Loyalty Score: ' + str(round(first['Loyalty'].mean(), 4)))\n",
    "print('2nd Place Contestants Loyalty Score: ' + str(round(second['Loyalty'].mean(), 4)))\n",
    "print('3rd Place Contestants Loyalty Score: ' + str(round(third['Loyalty'].mean(), 4)))\n",
    "print('4th Place Contestants Loyalty Score: ' + str(round(fourth['Loyalty'].mean(), 4)))\n",
    "print('Dumped Contestants Loyalty Score: ' + str(round(dumped['Loyalty'].mean(), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(data=data, x=\"Gender\", y=\"Loyalty\", hue=\"Finalist\", size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Islanders given a 1 automatically  doesn't seem like the best way to represent their loyalty. Can I scale this data another way?\n",
    "- I will probably give islanders with 0 couples a normalized value (median or mean?) so the scaling wont  be  so harsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of finalists vs non finalists\n",
    "finalist_location = finalists['Final_Location2'].value_counts()\n",
    "non_finalist_location = non_finalists['Final_Location2'].value_counts()\n",
    "\n",
    "colors = ['cyan', 'orchid', 'lightgreen', 'yellow', 'salmon', 'navy', 'orange', 'silver']\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.set_title('Finalist Location')\n",
    "ax1.pie(finalist_location, labels=finalist_location.index, colors = colors)\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.set_title('Non-Finalist Location')\n",
    "ax2.pie(non_finalist_location, labels=non_finalist_location.index, colors = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dummy columns for location\n",
    "\n",
    "location_dummies = pd.get_dummies(data['Final_Location2'])\n",
    "location_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat newly created columns with original dataframe\n",
    "data = pd.concat([data, location_dummies],axis=1)\n",
    "\n",
    "#Drop location column now\n",
    "# data = data.drop(columns=['Final_Location2'])\n",
    "# data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns i dont need\n",
    "column_drops = ['Name',\t'Ethnicity', 'Hair', 'Eye',\t'Hometown',\t'Region', 'Stay', 'Dumped', 'OG', 'Casa', 'Status', 'Couples', 'Location', 'Final_Location', 'Final_Location2']\n",
    "model_data = data.drop(columns=column_drops)\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send this model to a csv file to pull into modeling later\n",
    "model_data.to_csv(cwd + '/data/model_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splits: train/validate/test\n",
    "How to split? by Season?\n",
    "Train: 1-7: n-224 or ~76%\n",
    "Validate:8: n-36 or ~12%\n",
    "Test:9: n-35 or ~ 12%\n",
    "This split makes sense to me, since the model in practice will only be predicting finalists from one season (the current season), so it makes sense to train it on roughly 80% of available data, and then validate and test on single seasons so guage performance\n",
    "Then it is decided: model will be trained on data from Seasons 1 through 7\n",
    "                    Model performance will be validated on season 8 so I can tune paramaters based off of that\n",
    "                    Finally, I will test model using season 9 data\n",
    "                    Once season 10 has finished, I may remodel  and  add season 9 to validation set and use 10 as the test - At that point, I can also begin using the model to predict season 11 before results are known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://builtin.com/data-science/train-test-split \n",
    "\n",
    "good refernce above for this. shows how to do split AND how to import model of choice from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, validate, test splits\n",
    "def train_val_test(data, label):\n",
    "    #Split data into train,validate,test splits by season\n",
    "\n",
    "    # features = ['Name', 'Age', 'other features']\n",
    "    # x =  train.loc[:, features]\n",
    "    # y = train.loc[:, ['Finalist']]\n",
    "\n",
    "    train = data[data['Season'] <= 7]\n",
    "    validate = data[data['Season'] == 8]\n",
    "    test = data[data['Season'] == 9]\n",
    "\n",
    "    x_train = train.drop(columns=[label])\n",
    "    y_train = train[label]\n",
    "\n",
    "    x_validate = validate.drop(columns=[label])\n",
    "    y_validate = validate[label]\n",
    "\n",
    "    x_test = test.drop(columns=[label])\n",
    "    y_test = test[label]\n",
    "\n",
    "    print('Train Split: ', x_train.shape, '| Validate Split: ', x_validate.shape, '| Test Split: ', x_test.shape)\n",
    "\n",
    "    return x_train, y_train, x_validate, y_validate, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_validate, y_validate, x_test, y_test = train_val_test(model_data, 'Finalist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will likely leave for  another iteration\n",
    "\n",
    "- Will not scale, but  rather might normalize values for loyalty score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
